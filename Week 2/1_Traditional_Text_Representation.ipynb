{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Text Representation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:19.970512Z",
     "start_time": "2021-09-10T22:44:18.243858Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:19.986459Z",
     "start_time": "2021-09-10T22:44:19.972505Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = '''This will be followed by more of the same with the mist and fog clearing to give a day of unbroken sunshine \n",
    "everywhere on Tuesday and temperatures of between 22 and 27 degrees. It will warmest in the midlands. Temperatures \n",
    "could reach a September record for the century in Ireland, but are unlikely to surpass the 29.1 degrees recorded \n",
    "at Kildare’s Clongowes Wood College on September 1st, 1906. Tuesday, however, will be the last day of the sunshine \n",
    "with rain arriving across the country on Wednesday morning. Temperatures will remain as high as 24 degrees with the \n",
    "warmth punctuated by heavy showers.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.002383Z",
     "start_time": "2021-09-10T22:44:19.991385Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_sample = re.sub(\"[^A-Za-z0-9\\s.]\", \"\" , sample.replace('\\n', '').lower())\n",
    "cleaned_sample          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.034130Z",
     "start_time": "2021-09-10T22:44:20.003343Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_docs = [word_tokenize(doc) for doc in sent_tokenize(cleaned_sample)]\n",
    "print(tokens_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.050071Z",
     "start_time": "2021-09-10T22:44:20.035125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_id = {token: idx for idx, token in enumerate(set(word_tokenize(cleaned_sample)))}\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.065962Z",
     "start_time": "2021-09-10T22:44:20.052010Z"
    }
   },
   "outputs": [],
   "source": [
    "token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.081932Z",
     "start_time": "2021-09-10T22:44:20.068953Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = len(word_to_id)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.097861Z",
     "start_time": "2021-09-10T22:44:20.082907Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_sequences = []\n",
    "for each_seq in token_ids:\n",
    "    encoded_tokens = []\n",
    "    for each_token_id in each_seq:\n",
    "        a = np.zeros((1, num_words))\n",
    "        a[0, each_token_id] = 1\n",
    "        encoded_tokens.append(a)\n",
    "    encoded_sequences.append(encoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.162640Z",
     "start_time": "2021-09-10T22:44:20.098853Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.177593Z",
     "start_time": "2021-09-10T22:44:20.163638Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.193540Z",
     "start_time": "2021-09-10T22:44:20.178591Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.209484Z",
     "start_time": "2021-09-10T22:44:20.194541Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.fit([sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.225509Z",
     "start_time": "2021-09-10T22:44:20.211477Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "A mapping of terms to feature indices. Logic is similar to word_to_id in One Hot Encoding. Since the count vectorizer was initiated with instrcuting to remove the stopwords, the vocab size decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.241378Z",
     "start_time": "2021-09-10T22:44:20.228422Z"
    }
   },
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.257326Z",
     "start_time": "2021-09-10T22:44:20.244373Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_vector = vectorizer.transform([sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.273269Z",
     "start_time": "2021-09-10T22:44:20.260314Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.288223Z",
     "start_time": "2021-09-10T22:44:20.275267Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the array is the final vocab size (obtained with vectorizer.vocabulary_) and the counts of the tokens' in the text is displayed. For example, the tokne at 14th position appeared twice. In the vocab, at 14 we have the word 'day' and it appears twice in the sample text. Similarly, the word 'degrees' occur thrice, and is in the 15th position which indicates '3' in the cv_vector array.\n",
    "\n",
    "**Sample Text**\n",
    "\n",
    "'This will be followed by more of the same with the mist and fog clearing to give a **day** of unbroken sunshine everywhere on Tuesday and temperatures of between 22 and 27 degrees. It will warmest in the midlands. Temperatures could reach a September record for the century in Ireland, but are unlikely to surpass the 29.1 degrees recorded at Kildare’s Clongowes Wood College on September 1st, 1906. Tuesday, however, will be the last **day** of the sunshine with rain arriving across the country on Wednesday morning. Temperatures will remain as high as 24 degrees with the warmth punctuated by heavy showers.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.312060Z",
     "start_time": "2021-09-10T22:44:20.289221Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "cv_vector = vectorizer.fit_transform([sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.367992Z",
     "start_time": "2021-09-10T22:44:20.312060Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_vector = cv_vector.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "pd.DataFrame(cv_vector, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:44:20.383951Z",
     "start_time": "2021-09-10T22:44:20.369987Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "n = 3\n",
    "\n",
    "for grams in ngrams(word_tokenize(sample), n):\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:47:58.163646Z",
     "start_time": "2021-09-10T22:47:58.156612Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:47:58.975233Z",
     "start_time": "2021-09-10T22:47:58.961559Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(min_df=0., max_df=1., use_idf=True, stop_words=STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:47:59.685269Z",
     "start_time": "2021-09-10T22:47:59.670403Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vector = tf_idf.fit([sample])\n",
    "tfidf_vector.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:48:06.147049Z",
     "start_time": "2021-09-10T22:48:06.118321Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vector = tf_idf.transform([sample])\n",
    "tfidf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T22:48:08.074102Z",
     "start_time": "2021-09-10T22:48:08.021297Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = tf_idf.get_feature_names()\n",
    "pd.DataFrame(np.round(tfidf_vector.toarray(), 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer and Tf-idf on sent_tokenized(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:33.458832Z",
     "start_time": "2021-09-10T23:43:33.441870Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:34.226949Z",
     "start_time": "2021-09-10T23:43:34.213683Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() # Not using stopwords to demo the behavior with multiple token occurrence in one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:34.969456Z",
     "start_time": "2021-09-10T23:43:34.955502Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(sent_tokenize(cleaned_sample)) # here each element in the sent_tokenize list is a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:35.689121Z",
     "start_time": "2021-09-10T23:43:35.675131Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.transform(sent_tokenize(cleaned_sample)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:36.424344Z",
     "start_time": "2021-09-10T23:43:36.409493Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_vector = vectorizer.transform(sent_tokenize(cleaned_sample)).toarray()\n",
    "len(encoded_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:37.142565Z",
     "start_time": "2021-09-10T23:43:37.130701Z"
    }
   },
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:43:37.863517Z",
     "start_time": "2021-09-10T23:43:37.852057Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:00:10.255462Z",
     "start_time": "2021-09-10T23:00:10.236773Z"
    }
   },
   "source": [
    "**Note:**\n",
    "\n",
    "All document is of same length as the vocabulary.\n",
    "\n",
    "Explanation - \n",
    "1. 1st document, 1st word or position 0 = 0 ; because the word at index 0 is 1906 in the vocabulary. 1906 is not there in this document. Thus, it is zero.\n",
    "\n",
    "2. 1st document, 2nd word or position 1 = 0 ; because the word at index 1 is 1st in the vocabulary. 1st is not there in this document. Thus, it is zero.\n",
    "\n",
    "3. 1st document, 3rd word or position 2 = 1 ; because the word at index 2 is 22 in the vocabulary. 22 is there in this document. Thus, it is 1.\n",
    "\n",
    "3. 1st document, 8th word or position 7  = 3 ; because the word at index 7 is 'and' in the vocabulary. 'and' is there in this document thrice. Thus, it is 3.\n",
    "\n",
    "**Ref**\n",
    "\n",
    "'This will be followed by more of the same with the mist and fog clearing to give a day of unbroken sunshine \\neverywhere on Tuesday and temperatures of between 22 and 27 degrees.',\n",
    "\n",
    " 'It will warmest in the midlands.',\n",
    " \n",
    " 'Temperatures \\ncould reach a September record for the century in Ireland, but are unlikely to surpass the 29.1 degrees recorded \\nat Kildare’s Clongowes Wood College on September 1st, 1906.',\n",
    " \n",
    " 'Tuesday, however, will be the last day of the sunshine \\nwith rain arriving across the country on Wednesday morning.',\n",
    " 'Temperatures will remain as high as 24 degrees with the \\nwarmth punctuated by heavy showers.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:44:02.405502Z",
     "start_time": "2021-09-10T23:44:02.380534Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf.fit(sent_tokenize(cleaned_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:44:03.140217Z",
     "start_time": "2021-09-10T23:44:03.128266Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf.transform(sent_tokenize(cleaned_sample)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T23:44:03.898784Z",
     "start_time": "2021-09-10T23:44:03.853924Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = tf_idf.get_feature_names()\n",
    "pd.DataFrame(np.round(tf_idf.transform(sent_tokenize(cleaned_sample)).toarray(), 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
